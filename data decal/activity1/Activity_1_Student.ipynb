{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Welcome back! This is the activity 1 notebook.}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore how to implement some of the web scraping and text mining techniques that were covered in lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please uncomment the lines below if you don't have the package installed.\n",
    "#!pip install --user nltk\n",
    "#!pip install -U scikit-learn\n",
    "#!pip install pandas\n",
    "#!pip install -U matplotlib\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install requests\n",
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you learned in lecture, web scraping can be a valuable tool to build your own datasets.\n",
    "\n",
    "It is especially useful because it automates manual data entry from websites.\n",
    "\n",
    "Let's walk through an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to scrape data from this bookstore's website: http://books.toscrape.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://books.toscrape.com/\"\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = BeautifulSoup(r.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt to find the book urls of books on the frontpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.find(\"article\", class_ = \"product_pod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.find(\"article\", class_=\"product_pod\").div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.find(\"article\", class_=\"product_pod\").div.img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.find(\"article\", class_ = \"product_pod\").div.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content.find(\"article\", class_ = \"product_pod\").div.a.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we know what to look for, we can use the findAll() function to grab everything at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [y.div.a.get('href') for y in content.findAll(\"article\", class_ = \"product_pod\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now we can see that we grabbed all of the book urls on the front page. Now let's dig for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing we want to look for is the book category urls. (This way we can see what books are classified as.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the consistent structure that you saw---this is good for scraping purposes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_urls = [x.get('href') for x in content.findAll('a', href = re.compile('catalogue/category/books'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_urls[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we've found extra information that will be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put it together to get all the book information we can off this website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(url):\n",
    "    result = requests.get(url)\n",
    "    contents = BeautifulSoup(result.text, 'html.parser')\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating this function because we are going to be calling this operation many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_urls = [url]\n",
    "\n",
    "contents = parse(site_urls[0])\n",
    "\n",
    "# while we get two matches, this means that the webpage contains a 'previous' and a 'next' button\n",
    "# if there is only one button, this means that we are either on the first page or on the last page\n",
    "# we stop when we get to the last page\n",
    "\n",
    "while len(contents.findAll(\"a\", href=re.compile(\"page\"))) == 2 or len(site_urls) == 1:\n",
    "    \n",
    "    # get the new complete url by adding the fetched URL to the base URL (and removing the .html part of the base URL)\n",
    "    new_url = \"/\".join(site_urls[-1].split(\"/\")[:-1]) + \"/\" + contents.findAll(\"a\", href=re.compile(\"page\"))[-1].get(\"href\")\n",
    "    \n",
    "    # add the URL to the list\n",
    "    site_urls.append(new_url)\n",
    "    \n",
    "    # parse the next page\n",
    "    contents = parse(new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(site_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this solution stable? What if the catalog changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Let's get the book urls that we saw above for all pages.\n",
    "def get_books(url):\n",
    "    contents = parse(url)\n",
    "    #same logic as we saw above, except now getting the full url.\n",
    "    return([\"/\".join(url.split(\"/\")[:-1]) + \"/\" + x.div.a.get('href') for x in contents.findAll(\"article\", class_ = \"product_pod\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "for page in site_urls:\n",
    "    books.append(get_books(page))\n",
    "#need to flatten the final book list because get_books returns a list--creates list of lists\n",
    "books = [item for sublist in books for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have the book data. We can proceed to the final step here: getting all the data associated with each book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##First, I will collect the star ratings.\n",
    "ratings = []\n",
    "for url in books:\n",
    "    contents = parse(url)\n",
    "    ratings.append(contents.find('p', class_ = re.compile(\"star-rating\")).get(\"class\")[1]) \n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Now it's your turn! Figure out a way to collect price data and category data for each book. \n",
    "prices = []\n",
    "categories = []\n",
    "for url in books:\n",
    "    contents = parse(url)\n",
    "    #WRITE YOUR CODE HERE\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Figure out a way to collect the name and number of books in stock for each book.\n",
    "names = []\n",
    "num_in_stock = []\n",
    "for url in books:\n",
    "    contents = parse(url)\n",
    "    #WRITE YOUR CODE HERE\n",
    "print(names)\n",
    "print(num_in_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame({'name': names, 'amount in stock': num_in_stock, \n",
    "                            'prices (in British Pounds)': prices, 'star rating': ratings, \n",
    "                            'categories': categories})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO DO: Figure out how to collect product descriptions of each book. We will use this to do text mining. \n",
    "##HINT: Figure out how to handle exceptions or possible missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that you've seen me attempt to predict star ratings from the product descriptions with very low accuracy, try to \n",
    "#predict the genre of a book from the product description using a multinomial naive bayes model. (This should have more accuracy.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
